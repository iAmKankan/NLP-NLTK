{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "![deep](https://user-images.githubusercontent.com/12748752/134754236-8d5549c9-bd05-408d-ba63-0d56ab83c999.png)\n",
    "* The words which are generally filtered out before processing a Natural language are called stop words. \n",
    "* These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. \n",
    "* Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\n",
    "* Here, We will be using the **NLTK** library for removing stopwords.\n",
    "* NLTK comes with several stopword corpora, we will be using the English corpus. \n",
    "* This corpus contains a huge number of English stopwords like *a*, *the*, *be*, *for*, *do*, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "![light](https://user-images.githubusercontent.com/12748752/134754235-ae8efaf0-a27a-46f0-b439-b114cbb8cf3e.png)\n",
    "* Lets take a tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK library import \n",
    "![light](https://user-images.githubusercontent.com/12748752/134754235-ae8efaf0-a27a-46f0-b439-b114cbb8cf3e.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in `stop_words` it will be removed. \n",
    "> #### To optimize the speed of the stopword lookup we can convert `stop_words` to a `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### First we need to lowercase our text (because all of our stopwords are lowercased).\n",
    "> #### Then we use split our input text into a list of tokens (each token is a word seperated by a space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i’m', 'amazed', 'how', 'often', 'in', 'practice,', 'not', 'only', 'does', 'a', '@huggingface', 'nlp', 'model', 'solve', 'your', 'problem,', 'but', 'one', 'of', 'their', 'public', 'finetuned', 'checkpoints,', 'is', 'good', 'enough', 'for', 'the', 'job.', 'both', 'impressed,', 'and', 'a', 'little', 'disappointed', 'how', 'rarely', 'i', 'get', 'to', 'actually', 'train', 'a', 'model', 'that', 'matters', ':(']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweet.lower().split()\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### And now we can iterate through the list, we check if each word exists in `stop_words` - if it does we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
      "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "print(\"With stopwords:\", ' '.join(tweet))\n",
    "print(\"Without:\", ' '.join(tweet_no_stopwords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
