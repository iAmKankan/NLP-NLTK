{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "![deep](https://user-images.githubusercontent.com/12748752/134754236-8d5549c9-bd05-408d-ba63-0d56ab83c999.png)\n",
    "* A token is a single unit, or piece of information. \n",
    "* Typically in NLP we will find that models consume a *token*, which can represent a multitude of different things, such as:\n",
    "     * A word\n",
    "     * Part of a word\n",
    "     * A single character\n",
    "     * Puntuation mark *[,!-.]*\n",
    "     * Special token like *\\<URL\\>*, or *\\<NAME\\>*\n",
    "     * Model-specific special tokens, like *[CLS]* and *[SEP]* for BERT\n",
    "\n",
    "### Tweet Data\n",
    "![light](https://user-images.githubusercontent.com/12748752/134754235-ae8efaf0-a27a-46f0-b439-b114cbb8cf3e.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Spliting the paragraph into **word-level tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'NLP',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.split()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Spliting the paragraph into **character-level tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', '’', 'm', ' ', 'a', 'm', 'a', 'z', 'e', 'd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[char for char in tweet][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Character-level Tokens and Word-level Tokens\n",
    "![light](https://user-images.githubusercontent.com/12748752/134754235-ae8efaf0-a27a-46f0-b439-b114cbb8cf3e.png)\n",
    "\n",
    "> ### Advantage of Character-level Tokens over Word-level Tokens\n",
    "* The *'advantage'* of using **character-level embeddings** is that any models we train on this data will only need to remember all of the characters of the alphabet, punctuation characters, and spaces/newlines.\n",
    "* So the model **vocabulary** (list of all the tokens it *knows*) is very small. \n",
    "* Additionally if a new word appears outside of training, the model will still be able to digest it -\n",
    "* Whereas a **word-level embedding** model would not understand the new word and replace it with an *unknown token*.\n",
    "\n",
    "> ### Advantage of Word-level Tokens over Character-level Tokens  \n",
    "* Words carry a significant level of semantic meaning, and when we use character-level embedding this is mostly lost.\n",
    "* At a high-level we can view character-level embedding as being good for syntax, and word-level embedding as being better for semantics.\n",
    "* Although, in-reality, word-level embeddings almost always outpeform character-level embeddings.\n",
    "\n",
    "> ### Part-word tokens\n",
    "* Latest transformer models that text can be split into **part-word tokens**. \n",
    "* So for example, we may find that the word *'being'* is split into the tokens *\\[\"be\", \"-ing\"\\]*, or *'amazingly'* to *\\[\"amaz\", \"-ing\", \"-ly\"\\]*.\n",
    "\n",
    "* In addition to this, we typically seperate **punctuation** too, so in our previous example the tokens *'@huggingface'* and *'impressed,'* would become *\\[\"@\", \"huggingface\"\\]* and *\\[\"impressed\", \",\"\\]* respectively.\n",
    "\n",
    "* In our tweet we might want to find any token that begins with **@** and convert that token to **\\<USER\\>**, a unique token that we have specified to identify usernames in our tweets. This rule is logical as there are potentially millions of added tokens in our model if we include Twitter usernames, but the username doesn't tell our model anything about the meaning in the language of the text, for example:\n",
    "\n",
    "`@elonmusk thinks that the NLP models that @joebloggs made are super cool`\n",
    "\n",
    "* Has no real meaningful difference to our model as with:\n",
    "\n",
    "`@joebloggs thinks that the NLP models that @huggingface made are super cool`\n",
    "\n",
    "* The meaning and subsequent classification of both tweets should really be identical in our model. \n",
    "* So, it is logical to replace usernames with a single shared token. This approach is something that is commonly used for many different things such as:\n",
    "    * emails\n",
    "    * names/usernames\n",
    "    * URLs\n",
    "    * monetary values\n",
    "    * or any other numbers\n",
    "\n",
    "* But ofcourse we don't always want to do this for everything, this is simply a rough guide as to what we *may* want to tokenize.\n",
    "\n",
    "\n",
    "> ### BERT model-specific special tokens\n",
    "\n",
    "* For the BERT transformer model there are *five* special tokens that are used by the model, these are:\n",
    "\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Token</th>\n",
    "    <th class=\"tg-0lax\">Meaning</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"><span style=\"font-weight:bold\">[PAD]</span></td>\n",
    "    <td class=\"tg-0lax\">Padding token, allows us to maintain same-length sequences (512 tokens for Bert) even when different sized sentences are fed in</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"><span style=\"font-weight:bold\">[UNK]</span></td>\n",
    "    <td class=\"tg-0lax\">Used when a word is unknown to Bert</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"><span style=\"font-weight:bold\">[CLS]</span></td>\n",
    "    <td class=\"tg-0lax\">Appears at the start of every sequence</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"><span style=\"font-weight:bold\">[SEP]</span></td>\n",
    "    <td class=\"tg-0lax\">Indicates a seperator or end of sequence</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\"><span style=\"font-weight:bold\">[MASK]</span></td>\n",
    "    <td class=\"tg-0lax\">Used when masking tokens, for example in training with masked language modelling (MLM)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So if we take the *'NLP models'* tweet, processing that directly with our BERT specific tokens might look like this:\n",
    "\n",
    "```\n",
    "['[CLS]', '[UNK]', 'thinks', 'that', 'the', 'nlp', 'models', 'that', '[UNK]', 'made', 'are', 'super', 'cool', '[SEP]', '[PAD]', '[PAD]', ..., '[PAD]']\n",
    "```\n",
    "\n",
    "> ### Here, we have:\n",
    "\n",
    "* Applied **\\[CLS\\]** token to indicate the start of the sequence.\n",
    "* Both username tokens *@elonmusk* and *@joebloggs* were not 'known' words to BERT so BERT replaced them with unknown tokens **\\[UNK\\]**, alternatively we could have replaced these with our own special **user** tokens.\n",
    "* Added **\\[SEP\\]** token to the end of our sequence.\n",
    "* Padded the sequence upto the required length of 512 tokens *(required due to fixed input sequence length of BERT model)* using **\\[PAD\\]** tokens.\n",
    "\n",
    "* Different models will have different special tokens, but we will often that they are being used for similiar reasons.\n",
    "\n",
    "* That's everything on tokens for now, although we will cover tokenization in more depth (and the code too) for different models in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
