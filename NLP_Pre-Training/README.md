## Index
![deep](https://user-images.githubusercontent.com/12748752/134754236-8d5549c9-bd05-408d-ba63-0d56ab83c999.png)
* [Word Embedding (word2vec)](#word-embedding-word2vec)
## Natural Language Processing: Pretraining
![deep](https://user-images.githubusercontent.com/12748752/134754236-8d5549c9-bd05-408d-ba63-0d56ab83c999.png)

* Pretrained text representations can be fed to various deep learning architectures for different downstream natural language processing applications. 
<img src="https://user-images.githubusercontent.com/12748752/139561324-2b923a98-80bd-49f7-8f74-632563bab76f.png" />

## Word Embedding (word2vec)
![deep](https://user-images.githubusercontent.com/12748752/134754236-8d5549c9-bd05-408d-ba63-0d56ab83c999.png)
*  Word vectors are vectors used to represent words of Natural Languages.
* It can also be considered as feature vectors or representations of words.
* **The technique of mapping words to real vectors is called word embedding**.

### Avoid One-Hot Vectors
![light](https://user-images.githubusercontent.com/12748752/134754235-ae8efaf0-a27a-46f0-b439-b114cbb8cf3e.png)
